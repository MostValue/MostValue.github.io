---
title: "Creating Your Own MiniGPT"
date: 2024-04-10
excerpt: "How to build your own GPT Model from scratch!"
collection: blog
permalink: /blog/miniGPT
excerpt_separator: <!--more-->
toc: true
toc_label: "Unique Title"
toc_icon: "heart"  # corresponding Font Awesome icon name (without fa prefix)
header: 
  og_image: <img src="/assets/img/images/shakespeare-f2.jpg" alt="">
tags:
  - Deep Learning
  - GPT
  - Project
---

{% include toc.html %}

# Goal:

- Reproduce a mini version of ChatGPT from scratch

Opening and inspecting the text:


```python
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
```


```python
print("Length of input in characters:", len(text))
```

    Length of input in characters: 1115394



```python
# let's look at the first 100 characters
print(text[:100])
```

    First Citizen:
    Before we proceed any further, hear me speak.
    
    All:
    Speak, speak.
    
    First Citizen:
    You


Making a vocab size: list of all unique characters


```python
chars = sorted(list(set(text)))
vocab_size = len(chars)
print(''.join(chars))
print(vocab_size) # this is all the possible unique characters
```

    
     !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
    65


### Making a Simple Tokenizer:

Tokenizers can range in size
- e.g. subword tokens, goal to translate characters to integers
- Sentencepeice by google: encoding subwords
- Tiktoken from OpenAI

Tradeoff between vocab size


```python
# create a mapping from string to integers

s_to_i = {ch:i for i,ch in enumerate(chars)}
i_to_s = {i:ch for i,ch in enumerate(chars)}


encode = lambda s: [s_to_i[c] for c in s]
decode = lambda i: ''.join([i_to_s[n] for n in i])
```


```python
print(encode("Hello"))
print(decode(encode("Hello")))
```

    [20, 43, 50, 50, 53]
    Hello


## Loading in the Data


```python
# encoding the entire text dataset and storing it in a Tensor

import torch

data = torch.tensor(encode(text), dtype=torch.long)
print(data.shape, data.dtype)
print(data[:1000])
```

    torch.Size([1115394]) torch.int64
    tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,
            53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,
             1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,
            57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,
             6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,
            58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,
             1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,
            53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,
            57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,
             8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,
             1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,
            53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,
            47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,
             1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,
            50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,
            49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,
            47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,
            46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,
            43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,
            54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,
            47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,
             1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,
             1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,
             1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,
            47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,
            53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,
            58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,
            39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,
            47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,
            39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,
            46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,
             1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,
             1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,
            50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,
            56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,
            61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,
             1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,
            56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,
            50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,
             1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,
            58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,
            39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,
            40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,
            63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,
            53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,
            57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,
            11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,
            57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,
            43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,
             1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,
            56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,
            10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,
            61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,
            46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,
            52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,
            56, 43, 60, 43, 52, 45, 43,  8,  0,  0])



```python
#train test split

n = int(0.9 * len(data))
train_data = data[:n]
test_data = data[n:]
train.shape, test.shape
```




    (torch.Size([1003854]), torch.Size([111540]))




```python
block_size = 8
train_data[:block_size+1] # we use block size + 1 to ensure that there are 8 tokens in  our block (slicing is noninclusive) 
```




    tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])




```python
x = train_data[:block_size]
y = train_data[1:block_size+1]
for t in range(block_size):
    context = x[:t+1]
    target = y[t]
    print(f"when input is {context} the target: {target}")
```

    when input is tensor([18]) the target: 47
    when input is tensor([18, 47]) the target: 56
    when input is tensor([18, 47, 56]) the target: 57
    when input is tensor([18, 47, 56, 57]) the target: 58
    when input is tensor([18, 47, 56, 57, 58]) the target: 1
    when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
    when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
    when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58


- Ensures that the transformer can handle small block sizes, of possibly one
- uses chunks to ensure parallelisation


```python
torch.manual_seed(1337)
batch_size = 4 # how many independent sequences will we process in parallel?
block_size = 8 # what is the maximum context length for predictions?

def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,)) # generate batch size random numbers between 0 and len(data)- blocksize
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

xb, yb = get_batch('train')
print('inputs:')
print(xb.shape)
print(xb)
print('targets:')
print(yb.shape)
print(yb)

print('----')

for b in range(batch_size): # batch dimension
    for t in range(block_size): # time dimension
        context = xb[b, :t+1]
        target = yb[b,t]
        print(f"when input is {context.tolist()} the target: {target}")
```

    inputs:
    torch.Size([4, 8])
    tensor([[24, 43, 58,  5, 57,  1, 46, 43],
            [44, 53, 56,  1, 58, 46, 39, 58],
            [52, 58,  1, 58, 46, 39, 58,  1],
            [25, 17, 27, 10,  0, 21,  1, 54]])
    targets:
    torch.Size([4, 8])
    tensor([[43, 58,  5, 57,  1, 46, 43, 39],
            [53, 56,  1, 58, 46, 39, 58,  1],
            [58,  1, 58, 46, 39, 58,  1, 46],
            [17, 27, 10,  0, 21,  1, 54, 39]])
    ----
    when input is [24] the target: 43
    when input is [24, 43] the target: 58
    when input is [24, 43, 58] the target: 5
    when input is [24, 43, 58, 5] the target: 57
    when input is [24, 43, 58, 5, 57] the target: 1
    when input is [24, 43, 58, 5, 57, 1] the target: 46
    when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
    when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
    when input is [44] the target: 53
    when input is [44, 53] the target: 56
    when input is [44, 53, 56] the target: 1
    when input is [44, 53, 56, 1] the target: 58
    when input is [44, 53, 56, 1, 58] the target: 46
    when input is [44, 53, 56, 1, 58, 46] the target: 39
    when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
    when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
    when input is [52] the target: 58
    when input is [52, 58] the target: 1
    when input is [52, 58, 1] the target: 58
    when input is [52, 58, 1, 58] the target: 46
    when input is [52, 58, 1, 58, 46] the target: 39
    when input is [52, 58, 1, 58, 46, 39] the target: 58
    when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
    when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
    when input is [25] the target: 17
    when input is [25, 17] the target: 27
    when input is [25, 17, 27] the target: 10
    when input is [25, 17, 27, 10] the target: 0
    when input is [25, 17, 27, 10, 0] the target: 21
    when input is [25, 17, 27, 10, 0, 21] the target: 1
    when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
    when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39



```python
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)
```




    <torch._C.Generator at 0x7faf780c5630>




```python
class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)
        # returns the row of size vocab size for the token
    def forward(self, idx, targets=None):

        # idx and targets are both (B,T) tensor of integers
        logits = self.token_embedding_table(idx) # (B,T,C)
        return logits

m = BigramLanguageModel(vocab_size)
out = m(xb, yb)
print(out.shape) # Batch size of 4, by context of 8, by vocab / embedding size of 65 (B,T,C)
```

    torch.Size([4, 8, 65])


Loss:

- Cross entropy
- Measures the quality of logits with respect to targets
- how well are we predicting the next logit with respect to targets

Ideally, we want to measure 
```python
loss = F.cross_entropy(logits, targets)

```


```python
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

class BigramLanguageModel(nn.Module):

    def __init__(self, vocab_size):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)

    def forward(self, idx, targets=None):

        # idx and targets are both (B,T) tensor of integers
        logits = self.token_embedding_table(idx) # (B,T,C)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape # unpacking the shape
            logits = logits.view(B*T, C) #squish them all out into a two dimensions
            # this is to ensure that it conforms with loss
            
            targets = targets.view(B*T)
            #matches logits
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is the current size of context. Goal is to turn (B, T+1)
        # idx is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # get the predictions 
            logits, loss = self(idx) # goes into forward options
            # focus only on the last time step (the last vector)
            logits = logits[:, -1, :] # becomes (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

m = BigramLanguageModel(vocab_size)
logits, loss = m(xb, yb)
print(logits.shape)
print(loss)

```

    torch.Size([32, 65])
    tensor(4.8786, grad_fn=<NllLossBackward0>)


We see that the loss is slightly more than a random prediction $(-\ln{\frac{1}{65}})$) (what we would expect) so we are gussing wrong and the distribution has a bit of entropy


```python
idx = torch.zeros((1, 1))
idx # feed in newline character
```




    tensor([[0.]])




```python
m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=10)[0].tolist()
```




    [0, 53, 33, 6, 54, 62, 21, 14, 18, 13, 37]




```python
print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))
```

    
    pxZOmufBNa!oUAWa!wa!Ucn!
    dgqUxJ;I&uSqE&oT:zYCJjHNxyjFa-vSPIkqpf iNY.IF$gH'xqtRteh$J;aXlcxz?FU,V.bERf


This is garbage because we haven't trained our model yet

## Training the Model


```python
# create a PyTorch optimizer
optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3) #1e-4 for larger models
```


```python
batch_size = 32
for steps in range(10000): # increase number of steps for good results...

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = m(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
print(loss.item())

```

    2.619483470916748


Loss is improving - less random


```python
print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))
```

    
    
    he thiprth lutissss?
    Angoled nass! by mep,
    Wivexchowathy stos IOHURGet winomme medoimy-thundiorenoo





```python
# !jupyter nbconvert --to script dev_notebook.ipynb
```

    [NbConvertApp] Converting notebook dev_notebook.ipynb to script
    [NbConvertApp] Writing 6927 bytes to dev_notebook.py


# Small computational trick / example using masking


```python
# This is what we want:
# we want x[b,t] = mean_{i<=t}( x[b,i] )
# this is the simplest example, using weak and average

torch.manual_seed(1337)

B, T, C= 4,8,2 #batch, time, channels
x = torch.randn(B,T,C)

xbow = torch.zeros((B,T,C))
for b in range(B):
    for t in range(T):
        xprev = x[b,:t+1] # (t,C) This is everything up to and including the t-th token
        xbow[b,t] = torch.mean(xprev, 0) # average out over (t,C), t th dimension

```


```python
x[0]
```




    tensor([[ 0.1808, -0.0700],
            [-0.3596, -0.9152],
            [ 0.6258,  0.0255],
            [ 0.9545,  0.0643],
            [ 0.3612,  1.1679],
            [-1.3499, -0.5102],
            [ 0.2360, -0.2398],
            [-0.9211,  1.5433]])




```python
xbow[0] #first is the same, second is average of first and second
```




    tensor([[ 0.1808, -0.0700],
            [-0.0894, -0.4926],
            [ 0.1490, -0.3199],
            [ 0.3504, -0.2238],
            [ 0.3525,  0.0545],
            [ 0.0688, -0.0396],
            [ 0.0927, -0.0682],
            [-0.0341,  0.1332]])




```python
# toy example illustrating how matrix multiplication can be used for a "weighted aggregation"
torch.manual_seed(42)
a = torch.tril(torch.ones(3, 3)) #creates triangular matrix of ones
a = a / torch.sum(a, 1, keepdim=True) #prevents broadcasting? to ensure that the sum is one.
b = torch.randint(0,10,(3,2)).float()
c = a @ b
print('a=')
print(a)
print('--')
print('b=')
print(b)
print('--')
print('c=')
print(c)


# note that [1, 1, 0] selects the first 2 values in b etc...
```

    a=
    tensor([[1.0000, 0.0000, 0.0000],
            [0.5000, 0.5000, 0.0000],
            [0.3333, 0.3333, 0.3333]])
    --
    b=
    tensor([[2., 7.],
            [6., 4.],
            [6., 5.]])
    --
    c=
    tensor([[2.0000, 7.0000],
            [4.0000, 5.5000],
            [4.6667, 5.3333]])



```python
wei = torch.tril(torch.ones(T,T))
wei = wei / wei.sum(1, keepdim=True)
xbow2 = wei @ x # (TXT) @ (B,T,C) --> pytorch will recognise this is a batch  multiply and do (B,T,T) @ (B, T, C) -> (B,T,C)
# can think about this as B X (T,T) @ (T,C) ->(B,T,C)
torch.allclose(xbow, xbow2) # tells us that this is the same
```




    True



Summary:
- used batch aggregation to use matrix multiplication to get weighted sums

Alternative function:


```python
tril = torch.tril(torch.ones(T,T))
tril
```




    tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
            [1., 1., 0., 0., 0., 0., 0., 0.],
            [1., 1., 1., 0., 0., 0., 0., 0.],
            [1., 1., 1., 1., 0., 0., 0., 0.],
            [1., 1., 1., 1., 1., 0., 0., 0.],
            [1., 1., 1., 1., 1., 1., 0., 0.],
            [1., 1., 1., 1., 1., 1., 1., 0.],
            [1., 1., 1., 1., 1., 1., 1., 1.]])




```python
# Option 3 (used in self attention)
wei = torch.zeros((T,T))
wei = wei.masked_fill(tril == 0, float('-inf'))
wei = F.softmax(wei, dim=-1)
xbow3 = wei@x
torch.allclose(xbow, xbow3)
```




    True



## Notes:

- Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.
- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.
- Each example across batch dimension is of course processed completely independently and never "talk" to each other
- In an "encoder" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a "decoder" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.
- "self-attention" just means that the keys and values are produced from the same source as queries. In "cross-attention", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)
- "Scaled" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below
